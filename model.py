# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f2LFcrC6jVuEdJCcNSLdJYta38f9CiKa
"""

## Clone the git repo
# !git clone https://github.com/raunakdhawan/P4_Behavior_Cloning.git
# !ls -a P4_Behavior_Cloning

## Import the required modules
import csv
import cv2
import numpy as np
import datetime
import random
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Conv2D, Dropout, MaxPooling2D, Flatten, Activation, Dense, Cropping2D, Lambda
import os

## Path to the data
# CSV
data_log_csv = '/opt/carnd_p3/data/driving_log.csv'
# data_log_csv = './P4_Behavior_Cloning/data/driving_log.csv'

# Image
data_img = '/opt/carnd_p3/data/IMG/'
# data_img = './P4_Behavior_Cloning/data/IMG/'

# Read the data log from csv file
lines = []
data_log_csv = data_log_csv
with open(data_log_csv) as data_log:
    read_lines = csv.reader(data_log)
    for line in read_lines:
        lines.append(line)

# Crop and Resize the images
def pre_process_image(image):
    image_cropped = image[80:140,:]
    return cv2.cvtColor(cv2.resize(image_cropped, (32,32)), cv2.COLOR_BGR2RGB)

# Generator to yield images in batches
def generator_images(data, img_path, batchSize = 32, steering_offset=0.2):
    while True:
        data = shuffle(data)
        for i in range(0, len(data), int(batchSize/4)):
            X_batch = []
            y_batch = []
            details = data[i: i+int(batchSize/4)]
            for line in details:
                image_center_path = img_path + line[0].split('/')[-1]
                image_left_path = img_path + line[1].split('/')[-1]
                image_right_path = img_path + line[2].split('/')[-1]
              
                if not os.path.isfile(image_center_path) or not os.path.isfile(image_left_path) or not os.path.isfile(image_right_path):
                  continue
                
                # Read the center image
                image = cv2.imread(image_center_path)
                image = pre_process_image(image)
                steering_angle = float(line[3])
                X_batch.append(image)
                y_batch.append(steering_angle)
                
                # Horizontal flipped Image
                X_batch.append(np.fliplr(image))
                y_batch.append(-steering_angle)
                
                # Left Camera image
                image_l = cv2.imread(image_left_path)
                X_batch.append(pre_process_image(image_l))
                y_batch.append(steering_angle + steering_offset)
                
                # Right Camera Image
                image_r = cv2.imread(image_right_path)
                X_batch.append(pre_process_image(image_r))
                y_batch.append(steering_angle - steering_offset)
            
            # converting to numpy array
            X_batch = np.array(X_batch)
            y_batch = np.array(y_batch)
            yield shuffle(X_batch, y_batch)

# Create Training and Validation sets
training_data, validation_data = train_test_split(lines, test_size = 0.2)

# Create the model
model = Sequential()
model.add(Lambda(lambda x: x /255.0, input_shape=(32,32,3) ))  # With data normalization
model.add(Conv2D(15, (3, 3), activation="relu", strides=(2, 2)))
model.add(Dropout(0.4))
model.add(MaxPooling2D((2,2)))
model.add(Flatten())
model.add(Dense(1))

BATCH_SIZE = 32

# Compile and fit the model
model.compile('adam', 'mse')
model.fit_generator(generator_images(training_data, data_img), 
                    steps_per_epoch=len(training_data)*4, 
                    epochs=2, 
                    validation_data=generator_images(validation_data, data_img), 
                    validation_steps=len(validation_data))

#saving the model
model.save('model.h5')